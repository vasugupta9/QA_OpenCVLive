{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106edade-7a14-492f-92d4-4aca646485ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code reference - https://www.youtube.com/watch?v=2kSPbH4jWME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "309760f3-e3b3-4e5d-a865-2b106910f619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install ipywidgets\n",
    "# !brew install portaudio\n",
    "# !pip install pyaudio\n",
    "# !pip install vosk # optional if not using OpenAI speech-to-text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "56f71499-2b29-4258-ac95-32a1af54efec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "import pyaudio\n",
    "import json\n",
    "from vosk import Model, KaldiRecognizer\n",
    "import time\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os \n",
    "import wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "74a58c81-ee1b-4ec4-97ad-8c863dabfaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_transcript = \"\" # context used to answer the question "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "12ec23b3-2b5c-4f83-8d6c-d4b2c17c6c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find audio device index using this code\n",
    "# p = pyaudio.PyAudio()\n",
    "# for i in range(p.get_device_count()):\n",
    "#     print(p.get_device_info_by_index(i))\n",
    "\n",
    "# p.terminate()\n",
    "\n",
    "# after running above code for my mac - 'index': 0, 'structVersion': 2, 'name': 'MacBook Pro Microphone'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e969a033-2936-497a-9a92-0a92ebf031ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting various params \n",
    "AUDIO_FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "FRAME_RATE = 16000\n",
    "RECORD_SECONDS = 0.5\n",
    "device_index=0\n",
    "SAMPLE_SIZE = 2\n",
    "wav_filename = \"recording.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ceeb6214-a038-466f-876a-13390fccc43f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eca31a7606142e0a960b147804067de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Record', icon='microphone', style=ButtonStyle(), tooltip='Record')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "070331ab07024e0db50dab52838e9126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='warning', description='Stop', icon='stop', style=ButtonStyle(), tooltip='Stop')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce222b948ddf4a9f9332ab6bd665ec06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# transcription for context (optional transcription for question below)\n",
    "messages = Queue()\n",
    "recordings = Queue()\n",
    "\n",
    "record_button = widgets.Button(\n",
    "    description='Record',\n",
    "    disabled=False,\n",
    "    button_style='success',\n",
    "    tooltip='Record',\n",
    "    icon='microphone'\n",
    ")\n",
    "stop_button = widgets.Button(\n",
    "    description='Stop',\n",
    "    disabled=False,\n",
    "    button_style='warning',\n",
    "    tooltip='Stop',\n",
    "    icon='stop'\n",
    ")\n",
    "output = widgets.Output()\n",
    "\n",
    "def start_recording(data):\n",
    "    messages.put(True)\n",
    "    with output:\n",
    "        display(\"Starting...\")\n",
    "        record = Thread(target=record_microphone)\n",
    "        record.start()\n",
    "        transcribe = Thread(target=speech_recognition, args=(output,))\n",
    "        transcribe.start()\n",
    "\n",
    "def stop_recording(data):\n",
    "    with output:\n",
    "        messages.get()\n",
    "        display(\"Stopped.\")\n",
    "        display(\"Context transcript:\")\n",
    "        display(context_transcript)\n",
    "\n",
    "record_button.on_click(start_recording)\n",
    "stop_button.on_click(stop_recording)\n",
    "\n",
    "display(record_button, stop_button, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d5d497a9-f24f-4d65-a3a7-7456b8a8eb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_microphone(chunk=1024):\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=AUDIO_FORMAT,\n",
    "                    channels=CHANNELS,\n",
    "                    rate=FRAME_RATE,\n",
    "                    input=True,\n",
    "                    input_device_index=device_index,\n",
    "                    frames_per_buffer=chunk)\n",
    "    frames = []\n",
    "    while not messages.empty():\n",
    "        data = stream.read(chunk)\n",
    "        frames.append(data)\n",
    "        if len(frames) >= (FRAME_RATE * RECORD_SECONDS) / chunk:\n",
    "            recordings.put(frames.copy())\n",
    "            frames = []\n",
    "    if len(frames)>0:\n",
    "        recordings.put(frames.copy())\n",
    "        frames = []        \n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7cc3a980-2c0e-4ebb-ac3f-825588153cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI speech-to-text transcription \n",
    "_ = load_dotenv(find_dotenv()) # finds .env file and adds key-value pairs specified in .env to enviornment variables\n",
    "openai_api_key   = os.environ.get(\"OPENAI_API_KEY\") \n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "def get_transcription():\n",
    "    audio_file= open(wav_filename, \"rb\")\n",
    "    transcription = client.audio.transcriptions.create(model=\"whisper-1\", file=audio_file)\n",
    "    return transcription.text \n",
    "\n",
    "def write_frames_to_file(frames):\n",
    "    wf = wave.open(wav_filename, 'wb')\n",
    "    wf.setnchannels(CHANNELS)\n",
    "    wf.setsampwidth(SAMPLE_SIZE)\n",
    "    wf.setframerate(FRAME_RATE)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "    wf.close() \n",
    "\n",
    "# transcription using openAI speech-to-text\n",
    "def speech_recognition(output):\n",
    "    global context_transcript \n",
    "    while not messages.empty():\n",
    "        frames = recordings.get()\n",
    "        write_frames_to_file(frames)\n",
    "        transcription = get_transcription()\n",
    "        context_transcript += transcription\n",
    "        output.append_stdout(transcription)\n",
    "    while not recordings.empty():\n",
    "        frames = recordings.get()\n",
    "        write_frames_to_file(frames)\n",
    "        transcription = get_transcription()\n",
    "        context_transcript += transcription\n",
    "        output.append_stdout(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8f82c190-63ad-4288-8e7c-f24c0f2b69a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "890ac014-8e5f-4f86-8873-69754872bd72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "614fffcd-f7aa-4e0e-95fb-1d11d1ac92c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using vosk \n",
    "# model = Model(model_name=\"vosk-model-small-en-us-0.15\") # \"vosk-model-en-us-0.22\"\n",
    "# rec = KaldiRecognizer(model, FRAME_RATE)\n",
    "# def speech_recognition(output):\n",
    "#     while not messages.empty():\n",
    "#         frames = recordings.get()\n",
    "#         rec.AcceptWaveform(b''.join(frames))\n",
    "#         result = rec.Result()\n",
    "#         text = json.loads(result)[\"text\"]\n",
    "#         output.append_stdout(text)\n",
    "#         #time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
